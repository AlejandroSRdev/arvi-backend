/**
 * Gemini Adapter (Infrastructure Layer)
 *
 * MIGRADO DESDE: src/services/aiService.js (l√≠neas 29-66, 134-173)
 * REFACTORIZADO: 2025-12-30
 * ALINEADO CON: frontend-reference/services/ai_service_refactor2.dart
 *
 * IMPLEMENTA: domain/ports/IAIProvider.js
 *
 * RESPONSABILIDADES:
 * - Implementar contrato IAIProvider usando Google Generative AI SDK
 * - Traducir input del dominio ‚Üí llamada Gemini API
 * - Traducir respuesta Gemini ‚Üí formato esperado por dominio
 * - Calcular tokensUsed (aproximaci√≥n: text.length / 3.7)
 * - Calcular energyConsumed seg√∫n f√≥rmula original:
 *   ceil((responseTokens + promptTokens √ó 0.30) / 100)
 *
 * NO CONTIENE:
 * - L√≥gica de prompts (eso est√° en el frontend)
 * - Flujos multi-pasada (eso est√° en el frontend)
 * - Decisiones de "qu√© generar" (eso est√° en el frontend)
 *
 * COMPORTAMIENTO ORIGINAL PRESERVADO:
 * - C√°lculo de tokens Gemini: text.length / 3.7
 * - C√°lculo de energ√≠a: ceil((response + prompt√ó0.30) / 100)
 * - Conversi√≥n de mensajes a formato Gemini
 * - forceJson ‚Üí responseMimeType: 'application/json'
 * - temperature, maxOutputTokens seg√∫n opciones
 */

import { getModel } from './GeminiConfig.js';
import { IAIProvider } from '../../../domain/ports/IAIProvider.js';
import { sanitizeUserInput } from '../../../application/input/SanitizeUserInput.js';

/**
 * Calcular tokens para Gemini (aproximaci√≥n)
 *
 * EXTRACCI√ìN: src/services/aiService.js:38-41 (calculateGeminiTokens)
 *
 * F√≥rmula original: text.length / 3.7
 *
 * @param {string} text - Texto a calcular
 * @returns {number} - N√∫mero estimado de tokens
 */
function calculateGeminiTokens(text) {
  if (!text || typeof text !== 'string') return 0;
  return Math.round(text.length / 3.7);
}

/**
 * Calcular energ√≠a a consumir para Gemini
 *
 * EXTRACCI√ìN: src/services/aiService.js:57-66 (calculateGeminiEnergy)
 *
 * F√≥rmula original:
 * 1. tokens_prompt = calculateGeminiTokens(prompt)
 * 2. tokens_respuesta = calculateGeminiTokens(respuesta)
 * 3. total = respuesta + (prompt √ó 0.30)
 * 4. energia = ceil(total / 100)
 *
 * @param {string} prompt - Prompt enviado
 * @param {string} response - Respuesta recibida
 * @returns {number} - Energ√≠a a consumir
 */
function calculateGeminiEnergy(prompt, response) {
  const tokensPrompt = calculateGeminiTokens(prompt);
  const tokensRespuesta = calculateGeminiTokens(response);
  const totalTokens = Math.round(tokensRespuesta + (tokensPrompt * 0.30));
  const energia = Math.ceil(totalTokens / 100);

  console.log(`üìä [Gemini Energy] Prompt: ${tokensPrompt}t, Respuesta: ${tokensRespuesta}t, Total: ${totalTokens}t ‚Üí Energ√≠a: ${energia}`);

  return energia;
}

/**
 * Adapter de Gemini que implementa el port IAIProvider
 */
export class GeminiAdapter extends IAIProvider {
  /**
   * Llamada universal a Gemini
   *
   * EXTRACCI√ìN: src/services/aiService.js:134-173
   *
   * @param {string} userId - ID del usuario (no usado por Gemini SDK, solo para logs)
   * @param {Array<Object>} messages - Array de mensajes [{role: 'user'|'system'|'assistant', content: string}] (construidos por el frontend)
   * @param {Object} options - Opciones de configuraci√≥n
   * @param {string} options.model - Modelo espec√≠fico (default: 'gemini-2.5-flash')
   * @param {number} options.temperature - Temperatura 0.0-1.0 (default: 0.7)
   * @param {number} options.maxTokens - L√≠mite de tokens (default: 1500)
   * @param {boolean} options.forceJson - Forzar respuesta JSON (default: false)
   * @returns {Promise<Object>} {content, model, tokensUsed, energyConsumed}
   */
  async callAI(userId, messages, options = {}) {
    try {
      const {
        model = 'gemini-2.5-flash',
        temperature = 0.7,
        maxTokens = 1500,
        forceJson = false,
      } = options;

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:136
      console.log(`üß† [Gemini] Llamando modelo: ${model}`);

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:138
      const geminiModel = getModel(model);

      // Sanitize user input at the system boundary (before any processing)
      // Only user messages contain raw input; system/assistant messages are internal
      const sanitizedMessages = messages.map(m => {
        if (m.role === 'user') {
          return { ...m, content: sanitizeUserInput(m.content) };
        }
        return m;
      });

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:140-145
      // Convertir mensajes a formato Gemini
      const prompt = sanitizedMessages.map(m => {
        if (m.role === 'system') return `[SYSTEM INSTRUCTIONS]\n${m.content}`;
        if (m.role === 'assistant') return `[ASSISTANT]\n${m.content}`;
        return m.content;
      }).join('\n\n');

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:147-151
      const generationConfig = {
        temperature,
        maxOutputTokens: maxTokens,
        ...(forceJson && { responseMimeType: 'application/json' }),
      };

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:153-156
      const result = await geminiModel.generateContent({
        contents: [{ parts: [{ text: prompt }] }],
        generationConfig,
      });

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:158-159
      const content = result.response.text();
      const tokensUsed = calculateGeminiTokens(content);

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:161-163
      // ‚úÖ SOLO GEMINI CONSUME ENERG√çA (contenido creativo)
      // F√≥rmula original de Flutter: ceil((responseTokens + promptTokens √ó 0.30) / 100)
      const energyToConsume = calculateGeminiEnergy(prompt, content);

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:165-170
      const response = {
        content,
        model,
        tokensUsed,
        energyConsumed: energyToConsume,
      };

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:172
      console.log(`‚úÖ [Gemini] Respuesta recibida - Tokens: ${tokensUsed}, Energ√≠a: ${energyToConsume}`);

      return response;

    } catch (error) {
      console.error(`‚ùå [GeminiAdapter] Error en callAI: ${error.message}`);
      throw new Error(`Error al llamar a Gemini: ${error.message}`);
    }
  }

  /**
   * Llamada a IA con mapeo autom√°tico de modelo seg√∫n tipo de funci√≥n
   *
   * NOTA: Este m√©todo delega la selecci√≥n de modelo al use-case.
   * En la pr√°ctica, el use-case usa ModelSelectionPolicy para
   * determinar el modelo correcto antes de llamar a callAI.
   *
   * @param {string} userId - ID del usuario
   * @param {Array<Object>} messages - Array de mensajes [{role, content}] (construidos por el frontend)
   * @param {string} functionType - Tipo de funci√≥n (usado por ModelSelectionPolicy)
   * @returns {Promise<Object>} {content, model, tokensUsed, energyConsumed}
   */
  async callAIWithFunctionType(userId, messages, functionType) {
    // Gemini usa gemini-2.5-flash por defecto para todas las funciones
    // El use case determina el modelo apropiado v√≠a ModelSelectionPolicy antes de llamar
    return await this.callAI(userId, messages, {
      model: 'gemini-2.5-flash',
      temperature: 0.7,
      maxTokens: 1500,
      forceJson: false,
    });
  }
}

export default GeminiAdapter;

/**
 * OpenAI Adapter (Infrastructure Layer)
 *
 * MIGRADO DESDE: src/services/aiService.js (l√≠neas 106-133)
 * REFACTORIZADO: 2025-12-30
 * ALINEADO CON: frontend-reference/services/ai_service_refactor2.dart
 *
 * IMPLEMENTA: domain/ports/IAIProvider.js
 *
 * RESPONSABILIDADES:
 * - Implementar contrato IAIProvider usando OpenAI SDK
 * - Traducir input del dominio ‚Üí llamada OpenAI API
 * - Traducir respuesta OpenAI ‚Üí formato esperado por dominio
 * - Calcular tokensUsed desde completion.usage.total_tokens
 * - Energ√≠a SIEMPRE es 0 (GPT no consume energ√≠a del usuario)
 *
 * NO CONTIENE:
 * - L√≥gica de prompts (eso est√° en el frontend)
 * - Flujos multi-pasada (eso est√° en el frontend)
 * - Decisiones de "qu√© generar" (eso est√° en el frontend)
 *
 * COMPORTAMIENTO ORIGINAL PRESERVADO:
 * - forceJson ‚Üí response_format: { type: 'json_object' }
 * - energyConsumed SIEMPRE 0 para OpenAI
 * - temperature, maxTokens, model seg√∫n opciones
 */

import { openai } from './OpenAIConfig.js';
import { IAIProvider } from '../../../01domain/ports/IAIProvider.js';

/**
 * Adapter de OpenAI que implementa el port IAIProvider
 */
export class OpenAIAdapter extends IAIProvider {
  /**
   * Llamada universal a OpenAI
   *
   * EXTRACCI√ìN: src/services/aiService.js:106-133
   *
   * @param {string} userId - ID del usuario (no usado por OpenAI, solo para logs)
   * @param {Array<Object>} messages - Array de mensajes [{role: 'user'|'system'|'assistant', content: string}] (construidos por el frontend)
   * @param {Object} options - Opciones de configuraci√≥n
   * @param {string} options.model - Modelo espec√≠fico (default: 'gpt-4o-mini')
   * @param {number} options.temperature - Temperatura 0.0-1.0 (default: 0.7)
   * @param {number} options.maxTokens - L√≠mite de tokens (default: 1500)
   * @param {boolean} options.forceJson - Forzar respuesta JSON (default: false)
   * @returns {Promise<Object>} {content, model, tokensUsed, energyConsumed}
   */
  async callAI(userId, messages, options = {}) {
    try {
      const {
        model = 'gpt-4o-mini',
        temperature = 0.7,
        maxTokens = 1500,
        forceJson = false,
      } = options;

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:108
      console.log(`ü§ñ [OpenAI] Llamando modelo: ${model}`);

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:110-116
      // Llamada a OpenAI API con opciones exactas
      const completion = await openai.chat.completions.create({
        model,
        messages,
        temperature,
        max_tokens: maxTokens,
        ...(forceJson && { response_format: { type: 'json_object' } }),
      });

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:118-119
      const content = completion.choices[0].message.content;
      const tokensUsed = completion.usage.total_tokens;

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:121-129
      // ‚ö†Ô∏è GPT NO CONSUME ENERG√çA DEL USUARIO
      // GPT solo se usa para conversi√≥n a JSON (no contenido creativo para UI)
      const energyToConsume = 0;

      const response = {
        content,
        model: completion.model,
        tokensUsed,
        energyConsumed: 0, // GPT NO consume energ√≠a
      };

      // EXTRACCI√ìN EXACTA: src/services/aiService.js:132
      console.log(`‚úÖ [OpenAI] Respuesta recibida - Tokens: ${tokensUsed}, Energ√≠a: 0 (GPT no consume)`);

      return response;

    } catch (error) {
      console.error(`‚ùå [OpenAIAdapter] Error en callAI: ${error.message}`);
      throw new Error(`Error al llamar a OpenAI: ${error.message}`);
    }
  }

  /**
   * Llamada a IA con mapeo autom√°tico de modelo seg√∫n tipo de funci√≥n
   *
   * NOTA: OpenAI se usa t√≠picamente con modelo fijo gpt-4o-mini.
   * Este m√©todo implementa el contrato IAIProvider pero en la pr√°ctica
   * siempre usa el mismo modelo.
   *
   * @param {string} userId - ID del usuario
   * @param {Array<Object>} messages - Array de mensajes [{role, content}] (construidos por el frontend)
   * @param {string} functionType - Tipo de funci√≥n (ignorado, OpenAI usa modelo fijo)
   * @returns {Promise<Object>} {content, model, tokensUsed, energyConsumed}
   */
  async callAIWithFunctionType(userId, messages, functionType) {
    // OpenAI siempre usa gpt-4o-mini para todas las funciones
    return await this.callAI(userId, messages, {
      model: 'gpt-4o-mini',
      temperature: 0.0,
      maxTokens: 1500,
      forceJson: false,
    });
  }
}

export default OpenAIAdapter;
